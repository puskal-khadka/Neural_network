{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight are\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n"
     ]
    }
   ],
   "source": [
    "#code by puskal khadka and see comment to know more about how code is working\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_inp=np.array([[1,1,1],[0,0,1],[1,0,1],[0,1,1]])\n",
    "train_out=np.array([[1],[0],[1],[0]])\n",
    "#for weight i assign random value\n",
    "\n",
    "#here .seed(1) means if again in next time if i use .seed(1) then i get same random number\n",
    "##puskal ----- to know more about it see this answer:   https://stackoverflow.com/a/21494630\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "##to get random number from [a,b] i use (b - a) * random_sample() + a  which is supported by numpy\n",
    "# i use weight in the range [-1,1]\n",
    "weight= 2 * np.random.random((3,1)) -1\n",
    "\n",
    "print(\"weight are\")\n",
    "print(weight)\n",
    "\n",
    "\n",
    "#here i am using sigmoid activation function in this network\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1/(a+np.exp(-a))\n",
    "\n",
    "def derivative_sigmoid(a):\n",
    "    return a+ (1-a)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    predict_out=np.dot(train_inp, weight)\n",
    "    sigmoid_predict_out=sigmoid(predict_out)\n",
    "    err=train_out - sigmoid_predict_out\n",
    "    \n",
    "    #above sigmoid_predict_out give output with much error\n",
    "    \n",
    "    #now here i train data model by taking error and adjusting weight according to error to get minimum error\n",
    "    #this process is known as back propagation --puskal\n",
    "    update=err * derivative_sigmoid(sigmoid_predict_out)\n",
    "    #here new adjusted weight obtained at each iteration \n",
    "    weight+=np.dot(train_inp.T,update)\n",
    "    #.T is for transposing matrix ie.4*3 to 3*4 so that we can perform multiplication with 4 *1 outcome ajust value name as update to obatin  weight in the form of 3*1\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training output is\n",
      "[[0.99802496]\n",
      " [0.00203865]\n",
      " [0.9999898 ]\n",
      " [0.00192311]]\n"
     ]
    }
   ],
   "source": [
    "print(\"after training output is\")\n",
    "print(sigmoid_predict_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########puskal ##### Analysis of Result\n",
    "##############################################\n",
    "# here Actual output=1,0,1,0\n",
    "\n",
    "# And \n",
    "# while training without backpropagation (ie single iteration) i ge\n",
    "# predicted output=0.74,0.58,0.48,0.84  which is far from actual output\n",
    "\n",
    "# but after training model with dataset (for this i use 500 iteration) we ger\n",
    "# predicted output=0.99,0.002, 0.99, 0.001  ie. output=1,0,1,0\n",
    "\n",
    "#even we more train data we get data in same above form because here i use sigmoid activation function and in sigmoid because is 1 in + infinity, and 0 in -ve infinity\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
